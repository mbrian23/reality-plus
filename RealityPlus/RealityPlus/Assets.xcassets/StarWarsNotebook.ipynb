{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6940bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0dc2cb08494bd18f7dd97d4bf68400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1622497172487_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-19.ec2.internal:20888/proxy/application_1622497172487_0011/\" class=\"emr-proxy-link\" emr-resource=\"j-1AY88P8I3VLB1\n",
       "\" application-id=\"application_1622497172487_0011\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-29.ec2.internal:8042/node/containerlogs/container_1622497172487_0011_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, BooleanType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e4212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc7306686ad4060a7ad431c3fbd307d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "spark = SQLContext(sc)\n",
    "\n",
    "spark.sql(\"use umflix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de8d858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2cf0c3466c44969e93cd5d14f55b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_movies = spark.sql(\"select * from movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ea8935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7d26e37fe44465be1476e216e9228e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieid|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|   null|               title|            [genres]|\n",
      "|      1|    Toy Story (1995)|[Adventure|Animat...|\n",
      "|      2|      Jumanji (1995)|[Adventure|Childr...|\n",
      "|      3|Grumpier Old Men ...|    [Comedy|Romance]|\n",
      "|      4|Waiting to Exhale...|[Comedy|Drama|Rom...|\n",
      "|      5|Father of the Bri...|            [Comedy]|\n",
      "|      6|         Heat (1995)|[Action|Crime|Thr...|\n",
      "|      7|      Sabrina (1995)|    [Comedy|Romance]|\n",
      "|      8| Tom and Huck (1995)|[Adventure|Children]|\n",
      "|      9| Sudden Death (1995)|            [Action]|\n",
      "|     10|    GoldenEye (1995)|[Action|Adventure...|\n",
      "|     11| \"American President|      [ The (1995)\"]|\n",
      "|     12|Dracula: Dead and...|     [Comedy|Horror]|\n",
      "|     13|        Balto (1995)|[Adventure|Animat...|\n",
      "|     14|        Nixon (1995)|             [Drama]|\n",
      "|     15|Cutthroat Island ...|[Action|Adventure...|\n",
      "|     16|       Casino (1995)|       [Crime|Drama]|\n",
      "|     17|Sense and Sensibi...|     [Drama|Romance]|\n",
      "|     18|   Four Rooms (1995)|            [Comedy]|\n",
      "|     19|Ace Ventura: When...|            [Comedy]|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_movies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0db97a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d2dc46529f41f1aca9e629b011f097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ratings = spark.sql(\"select * from ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6e9c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae78c35b4e904fe8b936b53a4a29ec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userid: int, movieid: int, rating: float, timestamp: date]"
     ]
    }
   ],
   "source": [
    "df_ratings.withColumn(\"rating\",col(\"rating\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c970f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2b12f8d28a499bac353f9d5fe3c699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userid|movieid|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|  null|   null|  null|     null|\n",
      "|     1|    296|   5.0|     null|\n",
      "|     1|    306|   3.5|     null|\n",
      "|     1|    307|   5.0|     null|\n",
      "|     1|    665|   5.0|     null|\n",
      "|     1|    899|   3.5|     null|\n",
      "|     1|   1088|   4.0|     null|\n",
      "|     1|   1175|   3.5|     null|\n",
      "|     1|   1217|   3.5|     null|\n",
      "|     1|   1237|   5.0|     null|\n",
      "|     1|   1250|   4.0|     null|\n",
      "|     1|   1260|   3.5|     null|\n",
      "|     1|   1653|   4.0|     null|\n",
      "|     1|   2011|   2.5|     null|\n",
      "|     1|   2012|   2.5|     null|\n",
      "|     1|   2068|   2.5|     null|\n",
      "|     1|   2161|   3.5|     null|\n",
      "|     1|   2351|   4.5|     null|\n",
      "|     1|   2573|   4.0|     null|\n",
      "|     1|   2632|   5.0|     null|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0996528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafd7e0d656d48dc8a7a0ebac2e832d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userid|movieid|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|  null|   null|  null|     null|\n",
      "|     1|    296|   5.0|     null|\n",
      "|     1|    306|   3.5|     null|\n",
      "|     1|    307|   5.0|     null|\n",
      "|     1|    665|   5.0|     null|\n",
      "|     1|    899|   3.5|     null|\n",
      "|     1|   1088|   4.0|     null|\n",
      "|     1|   1175|   3.5|     null|\n",
      "|     1|   1217|   3.5|     null|\n",
      "|     1|   1237|   5.0|     null|\n",
      "|     1|   1250|   4.0|     null|\n",
      "|     1|   1260|   3.5|     null|\n",
      "|     1|   1653|   4.0|     null|\n",
      "|     1|   2011|   2.5|     null|\n",
      "|     1|   2012|   2.5|     null|\n",
      "|     1|   2068|   2.5|     null|\n",
      "|     1|   2161|   3.5|     null|\n",
      "|     1|   2351|   4.5|     null|\n",
      "|     1|   2573|   4.0|     null|\n",
      "|     1|   2632|   5.0|     null|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e9c326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a725005145cd4a6bba796ebfed030b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##df_ratings = df_ratings.filter(df_ratings.rating >= 4.0)\n",
    "\n",
    "##df_ratings.groupBy('userid').agg(collect_list('movieid')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb4fa760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783ccf8f26d44624b86fc5dfcde19dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ratings = df_ratings.withColumn(\"movieidornull\", F.when(df_ratings.rating>3.0, col(\"movieid\")).otherwise(None))\n",
    "data = df_ratings.groupBy('userid').agg(collect_list('movieidornull').alias('favorite_movies'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2fbafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70e0229dead434187c08f64a163b87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userid|     favorite_movies|\n",
      "+------+--------------------+\n",
      "|    12|[1, 16, 18, 29, 3...|\n",
      "|    65|[7153, 48516, 487...|\n",
      "|    76|[29, 32, 47, 50, ...|\n",
      "|    81|[50, 318, 527, 10...|\n",
      "|   122|[7, 11, 47, 62, 1...|\n",
      "|   126|[293, 296, 318, 3...|\n",
      "|   133|[6, 11, 16, 21, 2...|\n",
      "|   140|[110, 150, 161, 2...|\n",
      "|   148|[32, 47, 50, 110,...|\n",
      "|   177|[70, 95, 145, 368...|\n",
      "|   192|[50, 318, 413, 49...|\n",
      "|   209|[50, 111, 260, 29...|\n",
      "|   243|[47, 97, 111, 165...|\n",
      "|   300|[260, 586, 1036, ...|\n",
      "|   333|[2, 3, 27, 121, 1...|\n",
      "|   406|[260, 318, 364, 5...|\n",
      "|   417|[106, 213, 246, 2...|\n",
      "|   444|[318, 356, 1682, ...|\n",
      "|   481|[2, 47, 50, 104, ...|\n",
      "|   496|[199, 364, 1028, ...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cff63cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fc7b6736034dab9802ca23b91d4103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "starwars_movie_names = [\n",
    "    'Star Wars: Episode IV - A New Hope (1977)',\n",
    "     'Star Wars: Episode V - The Empire Strikes Back (1980)',\n",
    "     'Star Wars: Episode VI - Return of the Jedi (1983)',\n",
    "     'Star Wars: Episode I - The Phantom Menace (1999)',\n",
    "     'Star Wars: Episode II - Attack of the Clones (2002)',\n",
    "     'Star Wars: Episode III - Revenge of the Sith (2005)',\n",
    "     'Star Wars: Episode VII - The Force Awakens (2015)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2b7c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d31f6d37784b5bbbce22869e6e8da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260, 1196, 1210, 2628, 5378, 33493, 122886]"
     ]
    }
   ],
   "source": [
    "movie_list = [int(row[0]) for row in df_movies.filter(df_movies.title.isin(starwars_movie_names)).select('movieid').collect()]\n",
    "print(movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbdd5a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fd850896d54415a2fe5aadb3e5aaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def containsAny(string, array):\n",
    "    if len(string) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        contains = 1 if any(word in string for word in array) else 0\n",
    "        return contains\n",
    "    \n",
    "contains_udf = F.udf(containsAny, IntegerType())\n",
    "\n",
    "data = data.withColumn(\"label\", contains_udf(F.col(\"favorite_movies\"), F.array([F.lit(i) for i in movie_list])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39885a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eca899e9efb4dcd8cedb95ea9437cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+\n",
      "|userid|     favorite_movies|label|\n",
      "+------+--------------------+-----+\n",
      "|    12|[1, 16, 18, 29, 3...|    1|\n",
      "|    65|[7153, 48516, 487...|    0|\n",
      "|    76|[29, 32, 47, 50, ...|    0|\n",
      "|    81|[50, 318, 527, 10...|    0|\n",
      "|   122|[7, 11, 47, 62, 1...|    0|\n",
      "|   126|[293, 296, 318, 3...|    0|\n",
      "|   133|[6, 11, 16, 21, 2...|    0|\n",
      "|   140|[110, 150, 161, 2...|    0|\n",
      "|   148|[32, 47, 50, 110,...|    1|\n",
      "|   177|[70, 95, 145, 368...|    0|\n",
      "|   192|[50, 318, 413, 49...|    0|\n",
      "|   209|[50, 111, 260, 29...|    1|\n",
      "|   243|[47, 97, 111, 165...|    1|\n",
      "|   300|[260, 586, 1036, ...|    1|\n",
      "|   333|[2, 3, 27, 121, 1...|    0|\n",
      "|   406|[260, 318, 364, 5...|    1|\n",
      "|   417|[106, 213, 246, 2...|    1|\n",
      "|   444|[318, 356, 1682, ...|    0|\n",
      "|   481|[2, 47, 50, 104, ...|    1|\n",
      "|   496|[199, 364, 1028, ...|    0|\n",
      "+------+--------------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b085b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17597121c35f4d3bbd83b53ee7022304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train, df_test = data.randomSplit([1.0, 4.0], 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c0dd712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4db0a9617c4d4e9b8a1bda9bdd67b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.functions import array_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a500067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dd597f0eb641f4bf57bf3b2b7ced73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o399.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 63.0 failed 4 times, most recent failure: Lost task 8.3 in stage 63.0 (TID 1165) (ip-10-0-0-29.ec2.internal executor 12): java.lang.ArrayIndexOutOfBoundsException: 15\n",
      "\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:508)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2(RandomForest.scala:1041)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2$adapted(RandomForest.scala:1041)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplitsBySorting(RandomForest.scala:1054)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplits(RandomForest.scala:1025)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:282)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException: 15\n",
      "\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:508)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2(RandomForest.scala:1041)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2$adapted(RandomForest.scala:1041)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 161, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 114, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 161, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 335, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 332, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o399.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 63.0 failed 4 times, most recent failure: Lost task 8.3 in stage 63.0 (TID 1165) (ip-10-0-0-29.ec2.internal executor 12): java.lang.ArrayIndexOutOfBoundsException: 15\n",
      "\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:508)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2(RandomForest.scala:1041)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2$adapted(RandomForest.scala:1041)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplitsBySorting(RandomForest.scala:1054)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplits(RandomForest.scala:1025)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:282)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException: 15\n",
      "\tat org.apache.spark.ml.linalg.DenseVector.apply(Vectors.scala:508)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2(RandomForest.scala:1041)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findSplitsBySorting$2$adapted(RandomForest.scala:1041)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_vector = F.udf(array_to_vector, VectorUDT())\n",
    "\n",
    "sql_transformer = SQLTransformer(\n",
    "    statement = \"SELECT *, to_vector(favorite_movies) features FROM __THIS__\"\n",
    ")\n",
    "rf = RandomForestClassifier(numTrees=5, maxDepth=4, labelCol=\"label\", seed=42, featuresCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages = [sql_transformer, rf])\n",
    "\n",
    "\n",
    "model = pipeline.fit(df_train)\n",
    "df_test = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb71b34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13381298a39418ca1ccc36b7091fb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+--------------------+\n",
      "|userid|     favorite_movies|label|            features|\n",
      "+------+--------------------+-----+--------------------+\n",
      "|    12|[1, 16, 18, 29, 3...|    1|[1.0,16.0,18.0,29...|\n",
      "|    65|[7153, 48516, 487...|    0|[7153.0,48516.0,4...|\n",
      "|   122|[7, 11, 47, 62, 1...|    0|[7.0,11.0,47.0,62...|\n",
      "|   126|[293, 296, 318, 3...|    0|[293.0,296.0,318....|\n",
      "|   133|[6, 11, 16, 21, 2...|    0|[6.0,11.0,16.0,21...|\n",
      "|   140|[110, 150, 161, 2...|    0|[110.0,150.0,161....|\n",
      "|   148|[32, 47, 50, 110,...|    1|[32.0,47.0,50.0,1...|\n",
      "|   177|[70, 95, 145, 368...|    0|[70.0,95.0,145.0,...|\n",
      "|   209|[50, 111, 260, 29...|    1|[50.0,111.0,260.0...|\n",
      "|   243|[47, 97, 111, 165...|    1|[47.0,97.0,111.0,...|\n",
      "|   300|[260, 586, 1036, ...|    1|[260.0,586.0,1036...|\n",
      "|   333|[2, 3, 27, 121, 1...|    0|[2.0,3.0,27.0,121...|\n",
      "|   444|[318, 356, 1682, ...|    0|[318.0,356.0,1682...|\n",
      "|   481|[2, 47, 50, 104, ...|    1|[2.0,47.0,50.0,10...|\n",
      "|   496|[199, 364, 1028, ...|    0|[199.0,364.0,1028...|\n",
      "|   540|[1, 17, 34, 296, ...|    1|[1.0,17.0,34.0,29...|\n",
      "|   548|[1, 2, 6, 10, 31,...|    1|[1.0,2.0,6.0,10.0...|\n",
      "|   577|[50, 104, 110, 21...|    0|[50.0,104.0,110.0...|\n",
      "|   597|[1, 29, 32, 34, 3...|    1|[1.0,29.0,32.0,34...|\n",
      "|   599|[17, 26, 28, 215,...|    1|[17.0,26.0,28.0,2...|\n",
      "+------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9b961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
